#!/usr/bin/env python3
"""
Simple monitoring script to analyze API usage patterns from logs.
This helps identify potential abuse or unusual usage patterns.
"""

import re
import sys
from datetime import datetime, timedelta
from collections import defaultdict, Counter
import json

def parse_log_line(line):
    """Parse a single log line and extract relevant information"""
    # Expected format: "2025-08-28 10:30:45,123 - INFO - Request: POST /api/chat from 192.168.1.1 - User-Agent: ..."
    pattern = r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2}),\d{3} - INFO - Request: (\w+) ([\w/]+) from ([\d\.]+) - User-Agent: (.+)'
    
    match = re.match(pattern, line)
    if match:
        timestamp_str, method, path, ip, user_agent = match.groups()
        timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
        
        return {
            'timestamp': timestamp,
            'method': method,
            'path': path,
            'ip': ip,
            'user_agent': user_agent
        }
    return None

def analyze_logs(log_file_path):
    """Analyze logs and generate usage statistics"""
    
    requests = []
    
    try:
        with open(log_file_path, 'r') as f:
            for line in f:
                parsed = parse_log_line(line.strip())
                if parsed:
                    requests.append(parsed)
    except FileNotFoundError:
        print(f"Log file not found: {log_file_path}")
        return
    
    if not requests:
        print("No request logs found in the file.")
        return
    
    print("=== API Usage Analysis ===")
    print(f"Total requests analyzed: {len(requests)}")
    print(f"Time range: {requests[0]['timestamp']} to {requests[-1]['timestamp']}")
    print()
    
    # Requests per endpoint
    endpoint_counts = Counter(req['path'] for req in requests)
    print("=== Requests per Endpoint ===")
    for endpoint, count in endpoint_counts.most_common():
        print(f"{endpoint}: {count}")
    print()
    
    # Requests per IP
    ip_counts = Counter(req['ip'] for req in requests)
    print("=== Top IP Addresses ===")
    for ip, count in ip_counts.most_common(10):
        print(f"{ip}: {count}")
    print()
    
    # User agents analysis
    user_agents = Counter(req['user_agent'] for req in requests)
    print("=== User Agents ===")
    for ua, count in user_agents.most_common(5):
        print(f"{ua}: {count}")
    print()
    
    # Potential abuse detection
    print("=== Potential Issues ===")
    
    # Find IPs with unusually high request rates
    ip_request_times = defaultdict(list)
    for req in requests:
        ip_request_times[req['ip']].append(req['timestamp'])
    
    suspicious_ips = []
    for ip, timestamps in ip_request_times.items():
        if len(timestamps) > 100:  # More than 100 requests
            # Check if many requests came in a short time
            timestamps.sort()
            max_rate = 0
            for i in range(len(timestamps) - 10):
                window = timestamps[i:i+10]
                if window[-1] - window[0] < timedelta(minutes=1):
                    max_rate = 10  # 10 requests in less than a minute
                    break
            
            if max_rate > 5:
                suspicious_ips.append((ip, len(timestamps), max_rate))
    
    if suspicious_ips:
        print("Suspicious IP addresses (high request rate):")
        for ip, total, rate in suspicious_ips:
            print(f"  {ip}: {total} total requests, {rate} requests/minute peak")
    else:
        print("✓ No suspicious IP activity detected")
    
    # Check for unusual user agents
    suspicious_uas = []
    for ua, count in user_agents.items():
        ua_lower = ua.lower()
        if any(word in ua_lower for word in ['bot', 'spider', 'crawler', 'scan']) and 'googlebot' not in ua_lower:
            suspicious_uas.append((ua, count))
    
    if suspicious_uas:
        print("\nSuspicious User Agents:")
        for ua, count in suspicious_uas:
            print(f"  {ua}: {count} requests")
    else:
        print("✓ No suspicious user agents detected")
    
    # Requests per hour analysis
    hourly_requests = defaultdict(int)
    for req in requests:
        hour = req['timestamp'].replace(minute=0, second=0, microsecond=0)
        hourly_requests[hour] += 1
    
    print("\n=== Hourly Request Distribution ===")
    sorted_hours = sorted(hourly_requests.items())
    for hour, count in sorted_hours[-12:]:  # Show last 12 hours
        print(f"{hour.strftime('%Y-%m-%d %H:%M')}: {count} requests")

def generate_summary_report(log_file_path):
    """Generate a summary report in JSON format"""
    # This could be extended to generate reports for external monitoring tools
    pass

def main():
    if len(sys.argv) != 2:
        print("Usage: python monitor_api.py <log_file_path>")
        print("Example: python monitor_api.py /var/log/restaurant-ai/app.log")
        print("\nNote: This script analyzes logs generated by the Flask application.")
        print("Make sure logging is enabled and log rotation is configured.")
        sys.exit(1)
    
    log_file_path = sys.argv[1]
    analyze_logs(log_file_path)

if __name__ == "__main__":
    main()
